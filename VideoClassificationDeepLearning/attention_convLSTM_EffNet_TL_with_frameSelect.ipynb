{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1436057,"sourceType":"datasetVersion","datasetId":841381},{"sourceId":5401560,"sourceType":"datasetVersion","datasetId":3125063},{"sourceId":5417743,"sourceType":"datasetVersion","datasetId":3136992}],"dockerImageVersionId":30461,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport keras\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras import datasets, applications","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:42:01.330544Z","iopub.execute_input":"2023-04-27T15:42:01.331702Z","iopub.status.idle":"2023-04-27T15:42:11.953108Z","shell.execute_reply.started":"2023-04-27T15:42:01.331639Z","shell.execute_reply":"2023-04-27T15:42:11.951561Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#use this string for the path to the folder containing all the images in kaggle\n\nfolder_path = '/kaggle/input/ucf101/'\n\nclass_indices_file = open(folder_path+ 'UCF101TrainTestSplits-RecognitionTask/ucfTrainTestlist/classInd.txt', 'r')\nclass_index = {}\nfor line in class_indices_file:\n    class_ = line.split(' ')\n    class_index[class_[1].strip('\\n')] = int(class_[0])\n    \nclass_indices_file.close()\n\nprint(class_index)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:42:11.956717Z","iopub.execute_input":"2023-04-27T15:42:11.957994Z","iopub.status.idle":"2023-04-27T15:42:11.978683Z","shell.execute_reply.started":"2023-04-27T15:42:11.957944Z","shell.execute_reply":"2023-04-27T15:42:11.977271Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"{'ApplyEyeMakeup': 1, 'ApplyLipstick': 2, 'Archery': 3, 'BabyCrawling': 4, 'BalanceBeam': 5, 'BandMarching': 6, 'BaseballPitch': 7, 'Basketball': 8, 'BasketballDunk': 9, 'BenchPress': 10, 'Biking': 11, 'Billiards': 12, 'BlowDryHair': 13, 'BlowingCandles': 14, 'BodyWeightSquats': 15, 'Bowling': 16, 'BoxingPunchingBag': 17, 'BoxingSpeedBag': 18, 'BreastStroke': 19, 'BrushingTeeth': 20, 'CleanAndJerk': 21, 'CliffDiving': 22, 'CricketBowling': 23, 'CricketShot': 24, 'CuttingInKitchen': 25, 'Diving': 26, 'Drumming': 27, 'Fencing': 28, 'FieldHockeyPenalty': 29, 'FloorGymnastics': 30, 'FrisbeeCatch': 31, 'FrontCrawl': 32, 'GolfSwing': 33, 'Haircut': 34, 'Hammering': 35, 'HammerThrow': 36, 'HandstandPushups': 37, 'HandstandWalking': 38, 'HeadMassage': 39, 'HighJump': 40, 'HorseRace': 41, 'HorseRiding': 42, 'HulaHoop': 43, 'IceDancing': 44, 'JavelinThrow': 45, 'JugglingBalls': 46, 'JumpingJack': 47, 'JumpRope': 48, 'Kayaking': 49, 'Knitting': 50, 'LongJump': 51, 'Lunges': 52, 'MilitaryParade': 53, 'Mixing': 54, 'MoppingFloor': 55, 'Nunchucks': 56, 'ParallelBars': 57, 'PizzaTossing': 58, 'PlayingCello': 59, 'PlayingDaf': 60, 'PlayingDhol': 61, 'PlayingFlute': 62, 'PlayingGuitar': 63, 'PlayingPiano': 64, 'PlayingSitar': 65, 'PlayingTabla': 66, 'PlayingViolin': 67, 'PoleVault': 68, 'PommelHorse': 69, 'PullUps': 70, 'Punch': 71, 'PushUps': 72, 'Rafting': 73, 'RockClimbingIndoor': 74, 'RopeClimbing': 75, 'Rowing': 76, 'SalsaSpin': 77, 'ShavingBeard': 78, 'Shotput': 79, 'SkateBoarding': 80, 'Skiing': 81, 'Skijet': 82, 'SkyDiving': 83, 'SoccerJuggling': 84, 'SoccerPenalty': 85, 'StillRings': 86, 'SumoWrestling': 87, 'Surfing': 88, 'Swing': 89, 'TableTennisShot': 90, 'TaiChi': 91, 'TennisSwing': 92, 'ThrowDiscus': 93, 'TrampolineJumping': 94, 'Typing': 95, 'UnevenBars': 96, 'VolleyballSpiking': 97, 'WalkingWithDog': 98, 'WallPushups': 99, 'WritingOnBoard': 100, 'YoYo': 101}\n","output_type":"stream"}]},{"cell_type":"code","source":"req_classes = [[1,2], [8,9], [23,24], [30], [87], [70], [72], [100], [59,60,61,62,63,64,65,66,67], \n               [83],[98], [29], [85], [79], [80], [84], [51], [45], [77], [73], [44]]\n\nclass_names = []\nfor _ in req_classes:\n    classes = []\n    for __ in _:\n      classes.append([key for key in list(class_index.keys()) if class_index[key] == __][0])\n    class_names.append(classes)\nprint(class_names)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:42:11.981030Z","iopub.execute_input":"2023-04-27T15:42:11.981515Z","iopub.status.idle":"2023-04-27T15:42:11.993203Z","shell.execute_reply.started":"2023-04-27T15:42:11.981443Z","shell.execute_reply":"2023-04-27T15:42:11.991862Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"[['ApplyEyeMakeup', 'ApplyLipstick'], ['Basketball', 'BasketballDunk'], ['CricketBowling', 'CricketShot'], ['FloorGymnastics'], ['SumoWrestling'], ['PullUps'], ['PushUps'], ['WritingOnBoard'], ['PlayingCello', 'PlayingDaf', 'PlayingDhol', 'PlayingFlute', 'PlayingGuitar', 'PlayingPiano', 'PlayingSitar', 'PlayingTabla', 'PlayingViolin'], ['SkyDiving'], ['WalkingWithDog'], ['FieldHockeyPenalty'], ['SoccerPenalty'], ['Shotput'], ['SkateBoarding'], ['SoccerJuggling'], ['LongJump'], ['JavelinThrow'], ['SalsaSpin'], ['Rafting'], ['IceDancing']]\n","output_type":"stream"}]},{"cell_type":"code","source":"label_dict = {}\n\nlabel = 1\nfor _ in req_classes:\n    for id in _:\n        label_dict[id] = label\n    label += 1\n\nprint(label_dict)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:42:11.996845Z","iopub.execute_input":"2023-04-27T15:42:11.997760Z","iopub.status.idle":"2023-04-27T15:42:12.006508Z","shell.execute_reply.started":"2023-04-27T15:42:11.997710Z","shell.execute_reply":"2023-04-27T15:42:12.005028Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"{1: 1, 2: 1, 8: 2, 9: 2, 23: 3, 24: 3, 30: 4, 87: 5, 70: 6, 72: 7, 100: 8, 59: 9, 60: 9, 61: 9, 62: 9, 63: 9, 64: 9, 65: 9, 66: 9, 67: 9, 83: 10, 98: 11, 29: 12, 85: 13, 79: 14, 80: 15, 84: 16, 51: 17, 45: 18, 77: 19, 73: 20, 44: 21}\n","output_type":"stream"}]},{"cell_type":"code","source":"#considering only one test train split 01\ntrain_path_file = open(folder_path + \"UCF101TrainTestSplits-RecognitionTask/ucfTrainTestlist/trainlist01.txt\", \"r\")\ntrain_path_list = []\ntrain_y = []\n# print(next(train_path_file))\nlist_req_class = [i for l in req_classes for i in l]\nprint(list_req_class)\nfor _ in train_path_file:\n    path, class_num = _.split(' ')\n    if (int(class_num.strip('\\n')) in list_req_class):\n        train_path_list.append(path)\n        train_y.append(label_dict[int(class_num.strip('\\n'))])\n        #print(int(label_dict[class_num.strip('\\n')]))\n    \ntrain_path_file.close()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:42:12.008838Z","iopub.execute_input":"2023-04-27T15:42:12.009369Z","iopub.status.idle":"2023-04-27T15:42:12.043585Z","shell.execute_reply.started":"2023-04-27T15:42:12.009325Z","shell.execute_reply":"2023-04-27T15:42:12.042299Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"[1, 2, 8, 9, 23, 24, 30, 87, 70, 72, 100, 59, 60, 61, 62, 63, 64, 65, 66, 67, 83, 98, 29, 85, 79, 80, 84, 51, 45, 77, 73, 44]\n","output_type":"stream"}]},{"cell_type":"code","source":"len(train_path_list)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:42:12.045542Z","iopub.execute_input":"2023-04-27T15:42:12.046311Z","iopub.status.idle":"2023-04-27T15:42:12.055591Z","shell.execute_reply.started":"2023-04-27T15:42:12.046262Z","shell.execute_reply":"2023-04-27T15:42:12.054270Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"3035"},"metadata":{}}]},{"cell_type":"code","source":"test_path_file = open(folder_path + \"UCF101TrainTestSplits-RecognitionTask/ucfTrainTestlist/testlist01.txt\", \"r\")\ntest_path_list = []\ntest_y = []\n# print(next(test_path_file))\nfor _ in test_path_file:\n    path = _.strip('\\n')\n    a,b = path.split('/')\n    if (class_index[a] in list_req_class):\n        test_path_list.append(path)\n        test_y.append(label_dict[class_index[a]])\n\n\ntest_path_file.close()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:42:12.057924Z","iopub.execute_input":"2023-04-27T15:42:12.058742Z","iopub.status.idle":"2023-04-27T15:42:12.087777Z","shell.execute_reply.started":"2023-04-27T15:42:12.058696Z","shell.execute_reply":"2023-04-27T15:42:12.086446Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"len(test_path_list)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:42:12.089371Z","iopub.execute_input":"2023-04-27T15:42:12.090655Z","iopub.status.idle":"2023-04-27T15:42:12.099638Z","shell.execute_reply.started":"2023-04-27T15:42:12.090604Z","shell.execute_reply":"2023-04-27T15:42:12.098128Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"1214"},"metadata":{}}]},{"cell_type":"code","source":"# Using Keras's to_categorical method to convert labels into one-hot-encoded vectors\ntrain_y_hot = pd.get_dummies(train_y)\ntest_y_hot = pd.get_dummies(test_y)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:42:12.101391Z","iopub.execute_input":"2023-04-27T15:42:12.102017Z","iopub.status.idle":"2023-04-27T15:42:12.124332Z","shell.execute_reply.started":"2023-04-27T15:42:12.101960Z","shell.execute_reply":"2023-04-27T15:42:12.123146Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"labels = np.unique(np.array(train_y))\n\nsig_frame_dict = {}\nfor i in labels:\n    with open('/kaggle/input/train-hist-frames/'+ 'class_'+ str(i) +'_sig_frames.txt', 'r') as readfile:\n        contents = readfile.read()\n        lines = contents.splitlines()\n    for l in lines:\n        #print(l)\n        l_arr = l.split(\" \", 1)\n        sig_frame_dict['/kaggle/input/ucf101/UCF101/UCF-101/'+ l_arr[0]] = eval(l_arr[1])\n\n    with open('/kaggle/input/test-hist-frames/'+ 'test_class_'+ str(i) +'_sig_frames.txt', 'r') as read_testfile:\n        contents = read_testfile.read()\n        lines = contents.splitlines()\n    for l in lines:\n        l_arr = l.split(\" \", 1)\n        sig_frame_dict['/kaggle/input/ucf101/UCF101/UCF-101/' + l_arr[0]] = eval(l_arr[1])\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:42:12.129662Z","iopub.execute_input":"2023-04-27T15:42:12.130018Z","iopub.status.idle":"2023-04-27T15:42:12.653100Z","shell.execute_reply.started":"2023-04-27T15:42:12.129983Z","shell.execute_reply":"2023-04-27T15:42:12.651884Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"sig_frame_dict['/kaggle/input/ucf101/UCF101/UCF-101/ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi']","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:42:12.655775Z","iopub.execute_input":"2023-04-27T15:42:12.656108Z","iopub.status.idle":"2023-04-27T15:42:12.670010Z","shell.execute_reply.started":"2023-04-27T15:42:12.656076Z","shell.execute_reply":"2023-04-27T15:42:12.668774Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"[0, 1, 4, 6, 7, 12, 16, 22, 28, 33, 37, 42, 44, 48, 53, 58, 64, 72, 76, 84]"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ## **data loader**","metadata":{}},{"cell_type":"markdown","source":"## for skip interval/random/adaptive frame selection","metadata":{}},{"cell_type":"code","source":"\n# cnn_base = keras.applications.resnet.ResNet50(weights=\"imagenet\", include_top=False, input_shape= (224,224,3))\n\n# cnn_out = keras.layers.GlobalMaxPool2D()(cnn_base)\n# cnn = keras.Model(inputs=cnn_base.input, outputs=cnn_out)\n# cnn.trainable = False","metadata":{"execution":{"iopub.status.busy":"2023-04-26T09:17:30.999049Z","iopub.execute_input":"2023-04-26T09:17:31.000124Z","iopub.status.idle":"2023-04-26T09:17:31.007431Z","shell.execute_reply.started":"2023-04-26T09:17:31.000084Z","shell.execute_reply":"2023-04-26T09:17:31.006271Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# cnn.summary()","metadata":{"execution":{"iopub.status.busy":"2023-04-26T09:17:31.009044Z","iopub.execute_input":"2023-04-26T09:17:31.009747Z","iopub.status.idle":"2023-04-26T09:17:31.018659Z","shell.execute_reply.started":"2023-04-26T09:17:31.009710Z","shell.execute_reply":"2023-04-26T09:17:31.017514Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def generate_frames_from_videos(video_path, frame_selection = 'skip'):\n\n        #CAPTURING FRAMES after EVERY 20 FRAMES OF VIDEOS\n#         print(video_path, type(video_path))\n        video1 = cv2.VideoCapture(video_path)\n        #total_frames = video1.get(cv2.CAP_PROP_FRAME_COUNT)\n        \n        if (frame_selection == 'skip'):\n            frames_total = []\n            frames_arr = []\n            while video1.isOpened():\n                ret, frame = video1.read()\n\n                if not ret:  #reached end of video\n                    break\n                frames_total.append(frame)\n\n            video1.release()\n            total_frames = len(frames_total)\n            #print(total_frames)\n            interval = total_frames // 20\n\n            for _ in range (total_frames):\n                if _ % interval == 0:\n                    resized_frame = cv2.resize(frames_total[_], (224, 224))\n                    \n                    resized_frame = tf.keras.applications.resnet.preprocess_input(resized_frame)\n                    resized_frame = resized_frame.reshape(1,224,224,3)\n                    frames_arr.append(cnn.predict(resized_frame, verbose = 0))\n                if _ >= 19 * interval:\n                    break\n\n            frames_arr = np.squeeze(np.array(frames_arr), axis = 1)\n            return frames_arr\n        \n        \n        elif (frame_selection == 'random'):\n            \n            frames_total = []\n            frames_arr = []\n            while video1.isOpened():\n                ret, frame = video1.read()\n\n                if not ret:  #reached end of video\n                    break\n                frames_total.append(frame)\n\n            video1.release()\n            total_frames = len(frames_total)\n            rand_list = random.sample(range(0,total_frames),20)\n\n            for _ in sorted(rand_list):\n                resized_frame = cv2.resize(frames_total[_], (224, 224))\n                frames_arr.append(resized_frame)\n            return frames_arr\n        \n        \n        elif (frame_selection == 'hist_difference'):\n            frames_num = sig_frame_dict[video_path]\n            #print(len(frames_num))\n            frame_count = 0\n            frames_arr = []\n            \n            frames_total = []\n            while video1.isOpened():\n                ret, frame = video1.read()\n\n                if not ret:  #reached end of video\n                    break\n                frames_total.append(frame)\n                \n            total_frames = len(frames_total)\n            \n            actual = []\n            \n            for i in range(len(frames_num)):\n                ind = list(frames_num)[i]\n                if ind < total_frames:\n                    actual.append(ind)\n                else:\n                    int_ind = np.random.randint(0,total_frames)\n                    actual.append(int_ind)\n            if (len(actual)< 20):\n                actual = actual + list(np.random.randint(0,total_frames,20-len(actual)))\n                actual = sorted(actual)\n            for ind in actual:\n                resized_frame = cv2.resize(frames_total[int(ind)], (224, 224))\n                frames_arr.append(resized_frame)\n#                 resized_frame = tf.keras.applications.resnet.preprocess_input(resized_frame)\n#                 resized_frame = resized_frame.reshape(1,224,224,3)\n#                 frames_arr.append(cnn.predict(resized_frame, verbose = 0))\n\n            video1.release()\n#             frames_arr = np.squeeze(np.array(frames_arr), axis = 1)\n            return frames_arr\n            ","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:42:12.672266Z","iopub.execute_input":"2023-04-27T15:42:12.673017Z","iopub.status.idle":"2023-04-27T15:42:12.693120Z","shell.execute_reply.started":"2023-04-27T15:42:12.672945Z","shell.execute_reply":"2023-04-27T15:42:12.691748Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# from sklearn.utils import shuffle\n# train_path,train_labels=shuffle(train_path,train_labels, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:42:12.695336Z","iopub.execute_input":"2023-04-27T15:42:12.695858Z","iopub.status.idle":"2023-04-27T15:42:12.711571Z","shell.execute_reply.started":"2023-04-27T15:42:12.695801Z","shell.execute_reply":"2023-04-27T15:42:12.710308Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"z = generate_frames_from_videos('/kaggle/input/ucf101/UCF101/UCF-101/SalsaSpin/v_SalsaSpin_g14_c03.avi', 'hist_difference')\nnp.array(z).shape","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:42:12.713702Z","iopub.execute_input":"2023-04-27T15:42:12.714247Z","iopub.status.idle":"2023-04-27T15:42:12.961660Z","shell.execute_reply.started":"2023-04-27T15:42:12.714193Z","shell.execute_reply":"2023-04-27T15:42:12.960507Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"(20, 224, 224, 3)"},"metadata":{}}]},{"cell_type":"code","source":"folder_path = '/kaggle/input/ucf101/'\n\n#             print(str(folder_path + 'UCF101/UCF-101/' + str(video_path, 'UTF-8')))\nframe_arr = generate_frames_from_videos(str(folder_path + 'UCF101/UCF-101/' + str(train_path_list[1])), 'hist_difference')","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:42:12.963246Z","iopub.execute_input":"2023-04-27T15:42:12.963662Z","iopub.status.idle":"2023-04-27T15:42:13.022892Z","shell.execute_reply.started":"2023-04-27T15:42:12.963627Z","shell.execute_reply":"2023-04-27T15:42:13.021687Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"np.array(frame_arr).shape","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:42:13.024880Z","iopub.execute_input":"2023-04-27T15:42:13.025331Z","iopub.status.idle":"2023-04-27T15:42:13.034594Z","shell.execute_reply.started":"2023-04-27T15:42:13.025285Z","shell.execute_reply":"2023-04-27T15:42:13.033194Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"(20, 224, 224, 3)"},"metadata":{}}]},{"cell_type":"code","source":"import os","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:42:13.036881Z","iopub.execute_input":"2023-04-27T15:42:13.037936Z","iopub.status.idle":"2023-04-27T15:42:13.042952Z","shell.execute_reply.started":"2023-04-27T15:42:13.037896Z","shell.execute_reply":"2023-04-27T15:42:13.041635Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def video_data_generator(X=train_path_list, Y=train_y_hot, frame_selection = 'hist_difference'):\n        indices = np.arange(len(X))\n        np.random.shuffle(indices)\n        X_paths = [X[i] for i in indices]\n        labels = [Y[i] for i in indices]\n\n        folder_path = '/kaggle/input/ucf101/'\n        for i in range (len(X)):\n            video_path = X_paths[i]\n            label= labels[i]\n#             print(type(frame_selection))\n            frame_arr = generate_frames_from_videos(str(os.path.join(folder_path, \"UCF101/UCF-101/\", str(video_path, 'UTF-8'))), frame_selection.decode())\n            yield np.array(frame_arr), label\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:42:13.045212Z","iopub.execute_input":"2023-04-27T15:42:13.046270Z","iopub.status.idle":"2023-04-27T15:42:13.056104Z","shell.execute_reply.started":"2023-04-27T15:42:13.046173Z","shell.execute_reply":"2023-04-27T15:42:13.054978Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"### making an instance of the generator for hist_difference","metadata":{}},{"cell_type":"code","source":"#genertor converts every argument into bytes\n\ndataset_train = tf.data.Dataset.from_generator(\n video_data_generator,\n args = (train_path_list,train_y_hot,'hist_difference'),\n output_signature=(\n     tf.TensorSpec(shape=(20, 224,224,3), dtype=tf.float64),\n     tf.TensorSpec(shape=(21,), dtype=tf.int16)\n     )\n ).batch(batch_size = 8).prefetch(tf.data.AUTOTUNE)\ndataset_test = tf.data.Dataset.from_generator(\n video_data_generator,\n args = (test_path_list,test_y_hot,'hist_difference'),\n output_signature=(\n     tf.TensorSpec(shape=(20, 224,224,3), dtype=tf.float64),\n     tf.TensorSpec(shape=(21,))\n     )\n ).batch(batch_size = 8).prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:42:13.057715Z","iopub.execute_input":"2023-04-27T15:42:13.059244Z","iopub.status.idle":"2023-04-27T15:42:15.936676Z","shell.execute_reply.started":"2023-04-27T15:42:13.059197Z","shell.execute_reply":"2023-04-27T15:42:15.935442Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"sig_frame_dict[folder_path+ 'UCF101/UCF-101/' + 'Basketball/v_Basketball_g09_c01.avi']","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:42:15.938345Z","iopub.execute_input":"2023-04-27T15:42:15.938776Z","iopub.status.idle":"2023-04-27T15:42:15.946088Z","shell.execute_reply.started":"2023-04-27T15:42:15.938730Z","shell.execute_reply":"2023-04-27T15:42:15.944927Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"[0, 12, 24, 36, 37, 40, 43, 45, 47, 48, 56, 60, 65, 68, 72, 73, 75, 78, 80, 81]"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functional API","metadata":{}},{"cell_type":"markdown","source":"## Attention","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Dense, Permute, Multiply","metadata":{"execution":{"iopub.status.busy":"2023-04-25T15:14:12.581783Z","iopub.execute_input":"2023-04-25T15:14:12.582171Z","iopub.status.idle":"2023-04-25T15:14:12.587422Z","shell.execute_reply.started":"2023-04-25T15:14:12.582116Z","shell.execute_reply":"2023-04-25T15:14:12.586331Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"eff_net_ptmodel = tf.keras.applications.efficientnet_v2.EfficientNetV2L(\n    include_top=False,\n    weights='imagenet'\n)\neff_net_ptmodel.trainable = False","metadata":{"execution":{"iopub.status.busy":"2023-04-22T18:56:19.394506Z","iopub.execute_input":"2023-04-22T18:56:19.394930Z","iopub.status.idle":"2023-04-22T18:56:33.366559Z","shell.execute_reply.started":"2023-04-22T18:56:19.394889Z","shell.execute_reply":"2023-04-22T18:56:33.365481Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-l_notop.h5\n473176280/473176280 [==============================] - 3s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# encoding the images -- the model\neff_out = keras.layers.GlobalAveragePooling2D()(eff_net_ptmodel.output)\nencoder = keras.Model(inputs = eff_net_ptmodel.input, outputs= eff_out)\n\n\ninput_feature = keras.layers.Input(shape = [20, 224, 224, 3])\n\n\n#attention layer connecting the \nencoded_img =keras.layers.TimeDistributed(encoder)(input_feature)\n\nquery_img = keras.layers.TimeDistributed(keras.layers.Dense(128, activation = 'relu'))(encoded_img)\nkey_img = keras.layers.TimeDistributed(keras.layers.Dense(128, activation = 'relu'))(encoded_img)\n\natt_out = keras.layers.Attention()([query_img, key_img])\n\nconcat_layer = keras.layers.Concatenate()([att_out, encoded_img])\n\nencoded_sequence = keras.layers.Bidirectional(keras.layers.LSTM(128))(concat_layer)\n\nlstm_dense_td = keras.layers.Dense(64, activation = 'relu')(encoded_sequence)\n\noutputs = keras.layers.Dense(len(req_classes), activation=\"softmax\")(lstm_dense_td)\n\nmodel_effnet_att_lstm = keras.Model(input_feature, outputs)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T18:56:33.376122Z","iopub.execute_input":"2023-04-22T18:56:33.376792Z","iopub.status.idle":"2023-04-22T18:56:40.513400Z","shell.execute_reply.started":"2023-04-22T18:56:33.376751Z","shell.execute_reply":"2023-04-22T18:56:40.512188Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"model_effnet_att_lstm.summary()","metadata":{"execution":{"iopub.status.busy":"2023-04-21T20:07:04.997368Z","iopub.execute_input":"2023-04-21T20:07:04.999925Z","iopub.status.idle":"2023-04-21T20:07:05.161654Z","shell.execute_reply.started":"2023-04-21T20:07:04.999881Z","shell.execute_reply":"2023-04-21T20:07:05.160582Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Model: \"model_3\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_4 (InputLayer)           [(None, 20, 224, 22  0           []                               \n                                4, 3)]                                                            \n                                                                                                  \n time_distributed_7 (TimeDistri  (None, 20, 1280)    117746848   ['input_4[0][0]']                \n buted)                                                                                           \n                                                                                                  \n time_distributed_8 (TimeDistri  (None, 20, 128)     163968      ['time_distributed_7[0][0]']     \n buted)                                                                                           \n                                                                                                  \n time_distributed_9 (TimeDistri  (None, 20, 128)     163968      ['time_distributed_7[0][0]']     \n buted)                                                                                           \n                                                                                                  \n attention_2 (Attention)        (None, 20, 128)      0           ['time_distributed_8[0][0]',     \n                                                                  'time_distributed_9[0][0]']     \n                                                                                                  \n concatenate_2 (Concatenate)    (None, 20, 1408)     0           ['attention_2[0][0]',            \n                                                                  'time_distributed_7[0][0]']     \n                                                                                                  \n bidirectional_2 (Bidirectional  (None, 256)         1573888     ['concatenate_2[0][0]']          \n )                                                                                                \n                                                                                                  \n dense_9 (Dense)                (None, 64)           16448       ['bidirectional_2[0][0]']        \n                                                                                                  \n dense_10 (Dense)               (None, 21)           1365        ['dense_9[0][0]']                \n                                                                                                  \n==================================================================================================\nTotal params: 119,666,485\nTrainable params: 1,919,637\nNon-trainable params: 117,746,848\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 5, mode = 'min', restore_best_weights = True)","metadata":{"execution":{"iopub.status.busy":"2023-04-21T20:08:38.066095Z","iopub.execute_input":"2023-04-21T20:08:38.066592Z","iopub.status.idle":"2023-04-21T20:08:38.077443Z","shell.execute_reply.started":"2023-04-21T20:08:38.066541Z","shell.execute_reply":"2023-04-21T20:08:38.075764Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"model_effnet_att_lstm.compile(loss = 'categorical_crossentropy', optimizer = keras.optimizers.Adam()\n                     , metrics = [\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2023-04-21T20:09:30.088511Z","iopub.execute_input":"2023-04-21T20:09:30.089466Z","iopub.status.idle":"2023-04-21T20:09:30.126169Z","shell.execute_reply.started":"2023-04-21T20:09:30.089428Z","shell.execute_reply":"2023-04-21T20:09:30.125220Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"model_effnet_att_lstm.fit(dataset_train, epochs=5,validation_data=dataset_test)","metadata":{"execution":{"iopub.status.busy":"2023-04-21T20:10:14.363573Z","iopub.execute_input":"2023-04-21T20:10:14.364764Z","iopub.status.idle":"2023-04-21T20:58:50.206768Z","shell.execute_reply.started":"2023-04-21T20:10:14.364720Z","shell.execute_reply":"2023-04-21T20:58:50.205641Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Epoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"2023-04-21 20:10:49.314481: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_3/time_distributed_7/model_2/block1b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"95/95 [==============================] - 653s 6s/step - loss: 0.7489 - accuracy: 0.8033 - val_loss: 0.4530 - val_accuracy: 0.8674\nEpoch 2/5\n95/95 [==============================] - 557s 6s/step - loss: 0.1092 - accuracy: 0.9700 - val_loss: 0.4556 - val_accuracy: 0.8575\nEpoch 3/5\n95/95 [==============================] - 586s 6s/step - loss: 0.0399 - accuracy: 0.9924 - val_loss: 0.4734 - val_accuracy: 0.8789\nEpoch 4/5\n95/95 [==============================] - 560s 6s/step - loss: 0.0234 - accuracy: 0.9947 - val_loss: 0.5378 - val_accuracy: 0.8624\nEpoch 5/5\n95/95 [==============================] - 553s 6s/step - loss: 0.0140 - accuracy: 0.9980 - val_loss: 0.5254 - val_accuracy: 0.8748\n","output_type":"stream"},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x722aba508f50>"},"metadata":{}}]},{"cell_type":"code","source":"model_effnet_att_lstm.save(\"/kaggle/working/model_effnet_att_lstm1.h5\")","metadata":{"execution":{"iopub.status.busy":"2023-04-21T20:59:34.107968Z","iopub.execute_input":"2023-04-21T20:59:34.108671Z","iopub.status.idle":"2023-04-21T20:59:36.102060Z","shell.execute_reply.started":"2023-04-21T20:59:34.108634Z","shell.execute_reply":"2023-04-21T20:59:36.101017Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## convlstm attention lstm","metadata":{}},{"cell_type":"code","source":"modelconvlstm = Sequential()\n\n#channels_last --> format for the image (224,224,3)\n\n#smaller kernel size => capture slower motions\n#larger kernel size => capture faster motions\n\n\nmodelconvlstm = Sequential()\nmodelconvlstm.add(ConvLSTM2D(filters = 8, kernel_size = (5, 5), activation = 'tanh',data_format = \"channels_last\",\n                         recurrent_dropout=0.2, return_sequences=True, input_shape = (20, 224, 224, 3)))\n\nmodelconvlstm.add(MaxPooling3D(pool_size=(1, 2, 4), padding='valid', data_format='channels_last'))\nmodelconvlstm.add(TimeDistributed(Dropout(0.2)))\n\nmodelconvlstm.add(ConvLSTM2D(filters = 16, kernel_size = (7, 7), activation = 'tanh', data_format = \"channels_last\",\n                      recurrent_dropout=0.2, return_sequences=True))\n\nmodelconvlstm.add(MaxPooling3D(pool_size=(1, 2, 8), padding='valid', data_format='channels_last'))\nmodelconvlstm.add(TimeDistributed(Dropout(0.2)))\n\nmodelconvlstm.add(TimeDistributed(Flatten()))\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T15:14:20.584553Z","iopub.execute_input":"2023-04-25T15:14:20.585516Z","iopub.status.idle":"2023-04-25T15:14:21.026526Z","shell.execute_reply.started":"2023-04-25T15:14:20.585456Z","shell.execute_reply":"2023-04-25T15:14:21.025471Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"x = Input(shape = [20, 224,224, 3])\nencoded_img = modelconvlstm(x)\n\n\nquery_img =TimeDistributed(keras.layers.Dense(128, activation = 'relu'))(encoded_img)\nkey_img = TimeDistributed(keras.layers.Dense(128, activation = 'relu'))(encoded_img)\n\natt_out = Attention()([query_img, key_img])\n\nconcat_layer = Concatenate()([att_out, encoded_img])\n\nencoded_sequence = Bidirectional(keras.layers.LSTM(128))(concat_layer)\n\nlstm_dense_td = Dense(64, activation = 'relu')(encoded_sequence)\n\noutputs = Dense(len(req_classes), activation=\"softmax\")(lstm_dense_td)\n\nmodel_convlstm_att_lstm = keras.Model(x, outputs)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T15:14:21.677428Z","iopub.execute_input":"2023-04-25T15:14:21.678514Z","iopub.status.idle":"2023-04-25T15:14:22.457209Z","shell.execute_reply.started":"2023-04-25T15:14:21.678460Z","shell.execute_reply":"2023-04-25T15:14:22.456191Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"model_convlstm_att_lstm.summary()","metadata":{"execution":{"iopub.status.busy":"2023-04-25T15:14:23.396702Z","iopub.execute_input":"2023-04-25T15:14:23.397079Z","iopub.status.idle":"2023-04-25T15:14:23.430180Z","shell.execute_reply.started":"2023-04-25T15:14:23.397043Z","shell.execute_reply":"2023-04-25T15:14:23.429404Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 20, 224, 22  0           []                               \n                                4, 3)]                                                            \n                                                                                                  \n sequential_1 (Sequential)      (None, 20, 4992)     84160       ['input_1[0][0]']                \n                                                                                                  \n time_distributed_3 (TimeDistri  (None, 20, 128)     639104      ['sequential_1[0][0]']           \n buted)                                                                                           \n                                                                                                  \n time_distributed_4 (TimeDistri  (None, 20, 128)     639104      ['sequential_1[0][0]']           \n buted)                                                                                           \n                                                                                                  \n attention (Attention)          (None, 20, 128)      0           ['time_distributed_3[0][0]',     \n                                                                  'time_distributed_4[0][0]']     \n                                                                                                  \n concatenate (Concatenate)      (None, 20, 5120)     0           ['attention[0][0]',              \n                                                                  'sequential_1[0][0]']           \n                                                                                                  \n bidirectional (Bidirectional)  (None, 256)          5374976     ['concatenate[0][0]']            \n                                                                                                  \n dense_2 (Dense)                (None, 64)           16448       ['bidirectional[0][0]']          \n                                                                                                  \n dense_3 (Dense)                (None, 21)           1365        ['dense_2[0][0]']                \n                                                                                                  \n==================================================================================================\nTotal params: 6,755,157\nTrainable params: 6,755,157\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 5, mode = 'min', restore_best_weights = True)","metadata":{"execution":{"iopub.status.busy":"2023-04-25T15:14:25.421079Z","iopub.execute_input":"2023-04-25T15:14:25.421466Z","iopub.status.idle":"2023-04-25T15:14:25.427294Z","shell.execute_reply.started":"2023-04-25T15:14:25.421431Z","shell.execute_reply":"2023-04-25T15:14:25.425839Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"model_convlstm_att_lstm.compile(loss = 'categorical_crossentropy', optimizer = keras.optimizers.Adam()\n                     , metrics = [\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2023-04-25T15:14:27.647726Z","iopub.execute_input":"2023-04-25T15:14:27.648441Z","iopub.status.idle":"2023-04-25T15:14:27.664966Z","shell.execute_reply.started":"2023-04-25T15:14:27.648400Z","shell.execute_reply":"2023-04-25T15:14:27.664020Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"hist_model_convlstm_att_lstm = model_convlstm_att_lstm.fit(dataset_train, epochs=5,validation_data=dataset_test,callbacks = [early_stopping_callback])","metadata":{"execution":{"iopub.status.busy":"2023-04-25T15:14:30.590402Z","iopub.execute_input":"2023-04-25T15:14:30.590778Z","iopub.status.idle":"2023-04-25T16:01:10.189242Z","shell.execute_reply.started":"2023-04-25T15:14:30.590743Z","shell.execute_reply":"2023-04-25T16:01:10.188193Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Epoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"2023-04-25 15:14:37.677067: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel/sequential_1/conv_lstm2d/while/body/_1/model/sequential_1/conv_lstm2d/while/dropout_3/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"380/380 [==============================] - 592s 2s/step - loss: 2.5707 - accuracy: 0.3048 - val_loss: 2.3254 - val_accuracy: 0.3674\nEpoch 2/5\n380/380 [==============================] - 553s 1s/step - loss: 2.3648 - accuracy: 0.3367 - val_loss: 2.2973 - val_accuracy: 0.3534\nEpoch 3/5\n380/380 [==============================] - 549s 1s/step - loss: 2.1983 - accuracy: 0.3565 - val_loss: 2.2576 - val_accuracy: 0.3295\nEpoch 4/5\n380/380 [==============================] - 552s 1s/step - loss: 2.1451 - accuracy: 0.3710 - val_loss: 2.0971 - val_accuracy: 0.3913\nEpoch 5/5\n380/380 [==============================] - 535s 1s/step - loss: 1.9202 - accuracy: 0.4234 - val_loss: 2.0060 - val_accuracy: 0.3970\n","output_type":"stream"}]},{"cell_type":"code","source":"hist_model_convlstm_att_lstm = model_convlstm_att_lstm.fit(dataset_train, epochs=5,validation_data=dataset_test,callbacks = [early_stopping_callback])","metadata":{"execution":{"iopub.status.busy":"2023-04-25T16:15:26.063048Z","iopub.execute_input":"2023-04-25T16:15:26.063891Z","iopub.status.idle":"2023-04-25T17:01:18.541975Z","shell.execute_reply.started":"2023-04-25T16:15:26.063851Z","shell.execute_reply":"2023-04-25T17:01:18.540933Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"Epoch 1/5\n380/380 [==============================] - 559s 1s/step - loss: 1.8081 - accuracy: 0.4590 - val_loss: 2.0525 - val_accuracy: 0.3830\nEpoch 2/5\n380/380 [==============================] - 553s 1s/step - loss: 1.8212 - accuracy: 0.4445 - val_loss: 1.8859 - val_accuracy: 0.4325\nEpoch 3/5\n380/380 [==============================] - 537s 1s/step - loss: 1.5663 - accuracy: 0.5278 - val_loss: 1.9038 - val_accuracy: 0.4423\nEpoch 4/5\n380/380 [==============================] - 544s 1s/step - loss: 1.4827 - accuracy: 0.5437 - val_loss: 1.8767 - val_accuracy: 0.4399\nEpoch 5/5\n380/380 [==============================] - 547s 1s/step - loss: 1.3576 - accuracy: 0.5766 - val_loss: 1.8272 - val_accuracy: 0.4621\n","output_type":"stream"}]},{"cell_type":"code","source":"hist_model_convlstm_att_lstm = model_convlstm_att_lstm.fit(dataset_train, epochs=10,validation_data=dataset_test,callbacks = [early_stopping_callback])","metadata":{"execution":{"iopub.status.busy":"2023-04-25T17:02:41.683745Z","iopub.execute_input":"2023-04-25T17:02:41.684326Z","iopub.status.idle":"2023-04-25T17:57:14.109811Z","shell.execute_reply.started":"2023-04-25T17:02:41.684279Z","shell.execute_reply":"2023-04-25T17:57:14.108695Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Epoch 1/10\n380/380 [==============================] - 548s 1s/step - loss: 1.4331 - accuracy: 0.5638 - val_loss: 1.8157 - val_accuracy: 0.4893\nEpoch 2/10\n380/380 [==============================] - 557s 1s/step - loss: 1.2000 - accuracy: 0.6234 - val_loss: 1.8634 - val_accuracy: 0.4852\nEpoch 3/10\n380/380 [==============================] - 556s 1s/step - loss: 1.0476 - accuracy: 0.6689 - val_loss: 1.8803 - val_accuracy: 0.4802\nEpoch 4/10\n380/380 [==============================] - 544s 1s/step - loss: 1.0527 - accuracy: 0.6735 - val_loss: 2.1230 - val_accuracy: 0.4325\nEpoch 5/10\n380/380 [==============================] - 521s 1s/step - loss: 0.8630 - accuracy: 0.7318 - val_loss: 1.9897 - val_accuracy: 0.4720\nEpoch 6/10\n380/380 [==============================] - 535s 1s/step - loss: 0.8153 - accuracy: 0.7394 - val_loss: 2.7600 - val_accuracy: 0.3081\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## VGG  16 hist bilstm","metadata":{}},{"cell_type":"code","source":"vg_pret_model = tf.keras.applications.vgg16.VGG16(\n    include_top=False,\n    weights='imagenet'\n)\nvg_pret_model.trainable = False\nmodel_pret_x = Sequential()\nmodel_pret_x.add(TimeDistributed(vg_pret_model, input_shape= (20,224,224,3)))\n\nmodel_pret_x.add(TimeDistributed(GlobalMaxPool2D()))\n\nmodel_pret_x.add(Bidirectional(LSTM(128)))\n\nmodel_pret_x.add(Dense(len(req_classes), activation = 'softmax'))\n","metadata":{"execution":{"iopub.status.busy":"2023-04-26T09:17:43.470557Z","iopub.execute_input":"2023-04-26T09:17:43.471243Z","iopub.status.idle":"2023-04-26T09:17:45.005575Z","shell.execute_reply.started":"2023-04-26T09:17:43.471203Z","shell.execute_reply":"2023-04-26T09:17:45.004490Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n58889256/58889256 [==============================] - 0s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 5, mode = 'min', restore_best_weights = True)","metadata":{"execution":{"iopub.status.busy":"2023-04-26T09:17:53.570291Z","iopub.execute_input":"2023-04-26T09:17:53.571241Z","iopub.status.idle":"2023-04-26T09:17:53.576724Z","shell.execute_reply.started":"2023-04-26T09:17:53.571202Z","shell.execute_reply":"2023-04-26T09:17:53.575614Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"model_pret_x.compile(loss = 'categorical_crossentropy', optimizer = keras.optimizers.SGD()\n                     , metrics = [\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2023-04-26T09:18:00.502680Z","iopub.execute_input":"2023-04-26T09:18:00.503359Z","iopub.status.idle":"2023-04-26T09:18:00.521058Z","shell.execute_reply.started":"2023-04-26T09:18:00.503320Z","shell.execute_reply":"2023-04-26T09:18:00.519980Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"model_pret_x.summary()","metadata":{"execution":{"iopub.status.busy":"2023-04-26T09:18:09.917013Z","iopub.execute_input":"2023-04-26T09:18:09.917383Z","iopub.status.idle":"2023-04-26T09:18:09.943019Z","shell.execute_reply.started":"2023-04-26T09:18:09.917348Z","shell.execute_reply":"2023-04-26T09:18:09.942219Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n time_distributed (TimeDistr  (None, 20, 7, 7, 512)    14714688  \n ibuted)                                                         \n                                                                 \n time_distributed_1 (TimeDis  (None, 20, 512)          0         \n tributed)                                                       \n                                                                 \n bidirectional (Bidirectiona  (None, 256)              656384    \n l)                                                              \n                                                                 \n dense (Dense)               (None, 21)                5397      \n                                                                 \n=================================================================\nTotal params: 15,376,469\nTrainable params: 661,781\nNon-trainable params: 14,714,688\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"hist_pret_x = model_pret_x.fit(dataset_train, epochs=15,validation_data=dataset_test)","metadata":{"execution":{"iopub.status.busy":"2023-04-26T09:18:17.592555Z","iopub.execute_input":"2023-04-26T09:18:17.592925Z","iopub.status.idle":"2023-04-26T11:07:00.632502Z","shell.execute_reply.started":"2023-04-26T09:18:17.592891Z","shell.execute_reply":"2023-04-26T11:07:00.631461Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Epoch 1/15\n380/380 [==============================] - 476s 1s/step - loss: 2.0430 - accuracy: 0.4244 - val_loss: 1.7424 - val_accuracy: 0.4959\nEpoch 2/15\n380/380 [==============================] - 437s 1s/step - loss: 1.3605 - accuracy: 0.6471 - val_loss: 1.3893 - val_accuracy: 0.6516\nEpoch 3/15\n380/380 [==============================] - 422s 1s/step - loss: 1.0011 - accuracy: 0.7881 - val_loss: 1.0957 - val_accuracy: 0.6928\nEpoch 4/15\n380/380 [==============================] - 413s 1s/step - loss: 0.6985 - accuracy: 0.8774 - val_loss: 0.9156 - val_accuracy: 0.7578\nEpoch 5/15\n380/380 [==============================] - 423s 1s/step - loss: 0.5472 - accuracy: 0.9137 - val_loss: 0.8744 - val_accuracy: 0.7718\nEpoch 6/15\n380/380 [==============================] - 443s 1s/step - loss: 0.4439 - accuracy: 0.9367 - val_loss: 0.7824 - val_accuracy: 0.7891\nEpoch 7/15\n380/380 [==============================] - 422s 1s/step - loss: 0.3292 - accuracy: 0.9638 - val_loss: 0.7340 - val_accuracy: 0.7924\nEpoch 8/15\n380/380 [==============================] - 418s 1s/step - loss: 0.2604 - accuracy: 0.9773 - val_loss: 0.6747 - val_accuracy: 0.8040\nEpoch 9/15\n380/380 [==============================] - 426s 1s/step - loss: 0.2005 - accuracy: 0.9839 - val_loss: 0.6757 - val_accuracy: 0.8081\nEpoch 10/15\n380/380 [==============================] - 388s 1s/step - loss: 0.1639 - accuracy: 0.9911 - val_loss: 0.6333 - val_accuracy: 0.8196\nEpoch 11/15\n380/380 [==============================] - 398s 1s/step - loss: 0.1259 - accuracy: 0.9964 - val_loss: 0.6347 - val_accuracy: 0.8213\nEpoch 12/15\n380/380 [==============================] - 390s 1s/step - loss: 0.0999 - accuracy: 0.9977 - val_loss: 0.6028 - val_accuracy: 0.8122\nEpoch 13/15\n380/380 [==============================] - 418s 1s/step - loss: 0.0794 - accuracy: 0.9997 - val_loss: 0.5967 - val_accuracy: 0.8254\nEpoch 14/15\n380/380 [==============================] - 397s 1s/step - loss: 0.0657 - accuracy: 0.9993 - val_loss: 0.5717 - val_accuracy: 0.8361\nEpoch 15/15\n380/380 [==============================] - 399s 1s/step - loss: 0.0554 - accuracy: 1.0000 - val_loss: 0.5645 - val_accuracy: 0.8377\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eff_net_ptmodel = tf.keras.applications.efficientnet_v2.EfficientNetV2L(\n    include_top=False,\n    weights='imagenet'\n)\neff_net_ptmodel.trainable = False","metadata":{"execution":{"iopub.status.busy":"2023-04-27T15:42:25.828926Z","iopub.execute_input":"2023-04-27T15:42:25.829437Z","iopub.status.idle":"2023-04-27T15:42:41.441065Z","shell.execute_reply.started":"2023-04-27T15:42:25.829389Z","shell.execute_reply":"2023-04-27T15:42:41.439547Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-l_notop.h5\n473176280/473176280 [==============================] - 3s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# encoding the images -- the model\neff_out = keras.layers.GlobalAveragePooling2D()(eff_net_ptmodel.output)\nencoder = keras.Model(inputs = eff_net_ptmodel.input, outputs= eff_out)\n\n\ninput_feature = keras.layers.Input(shape = [20, 224, 224, 3])\n\n\n#attention layer connecting the \nencoded_img =keras.layers.TimeDistributed(encoder)(input_feature)\n\nlstm_out = keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences = True))(encoded_img)\n\nconcat_layer = keras.layers.Concatenate()([lstm_out, encoded_img])\n\n\nquery_img = keras.layers.TimeDistributed(keras.layers.Dense(128, activation = 'relu'))(concat_layer)\nkey_img = keras.layers.TimeDistributed(keras.layers.Dense(128, activation = 'relu'))(concat_layer)\n\natt_out = keras.layers.Attention()([query_img, key_img])\n\nflatten = keras.layers.Flatten()(att_out)\n\nlstm_dense_td = keras.layers.Dense(64, activation = 'relu')(flatten)\n\noutputs = keras.layers.Dense(len(req_classes), activation=\"softmax\")(lstm_dense_td)\n\nmodel_effnet_lstm_att = keras.Model(input_feature, outputs)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T16:11:48.036042Z","iopub.execute_input":"2023-04-27T16:11:48.036683Z","iopub.status.idle":"2023-04-27T16:11:55.370092Z","shell.execute_reply.started":"2023-04-27T16:11:48.036640Z","shell.execute_reply":"2023-04-27T16:11:55.369010Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"model_effnet_lstm_att.summary()","metadata":{"execution":{"iopub.status.busy":"2023-04-27T16:11:55.372092Z","iopub.execute_input":"2023-04-27T16:11:55.372485Z","iopub.status.idle":"2023-04-27T16:11:55.486927Z","shell.execute_reply.started":"2023-04-27T16:11:55.372427Z","shell.execute_reply":"2023-04-27T16:11:55.485625Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Model: \"model_9\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_7 (InputLayer)           [(None, 20, 224, 22  0           []                               \n                                4, 3)]                                                            \n                                                                                                  \n time_distributed_11 (TimeDistr  (None, 20, 1280)    117746848   ['input_7[0][0]']                \n ibuted)                                                                                          \n                                                                                                  \n bidirectional_5 (Bidirectional  (None, 20, 256)     1442816     ['time_distributed_11[0][0]']    \n )                                                                                                \n                                                                                                  \n concatenate_5 (Concatenate)    (None, 20, 1536)     0           ['bidirectional_5[0][0]',        \n                                                                  'time_distributed_11[0][0]']    \n                                                                                                  \n time_distributed_12 (TimeDistr  (None, 20, 128)     196736      ['concatenate_5[0][0]']          \n ibuted)                                                                                          \n                                                                                                  \n time_distributed_13 (TimeDistr  (None, 20, 128)     196736      ['concatenate_5[0][0]']          \n ibuted)                                                                                          \n                                                                                                  \n attention_3 (Attention)        (None, 20, 128)      0           ['time_distributed_12[0][0]',    \n                                                                  'time_distributed_13[0][0]']    \n                                                                                                  \n flatten_2 (Flatten)            (None, 2560)         0           ['attention_3[0][0]']            \n                                                                                                  \n dense_14 (Dense)               (None, 64)           163904      ['flatten_2[0][0]']              \n                                                                                                  \n dense_15 (Dense)               (None, 21)           1365        ['dense_14[0][0]']               \n                                                                                                  \n==================================================================================================\nTotal params: 119,748,405\nTrainable params: 2,001,557\nNon-trainable params: 117,746,848\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 5, mode = 'min', restore_best_weights = True)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T16:12:01.864945Z","iopub.execute_input":"2023-04-27T16:12:01.865982Z","iopub.status.idle":"2023-04-27T16:12:01.873264Z","shell.execute_reply.started":"2023-04-27T16:12:01.865923Z","shell.execute_reply":"2023-04-27T16:12:01.871806Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"model_effnet_lstm_att.compile(loss = 'categorical_crossentropy', optimizer = keras.optimizers.Adam()\n                     , metrics = [\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2023-04-27T16:12:09.144168Z","iopub.execute_input":"2023-04-27T16:12:09.144605Z","iopub.status.idle":"2023-04-27T16:12:09.175406Z","shell.execute_reply.started":"2023-04-27T16:12:09.144567Z","shell.execute_reply":"2023-04-27T16:12:09.174276Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"hist=model_effnet_lstm_att.fit(dataset_train, epochs=10,validation_data=dataset_test)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T16:14:27.139950Z","iopub.execute_input":"2023-04-27T16:14:27.140750Z","iopub.status.idle":"2023-04-27T17:56:26.702799Z","shell.execute_reply.started":"2023-04-27T16:14:27.140708Z","shell.execute_reply":"2023-04-27T17:56:26.701424Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"2023-04-27 16:15:05.381158: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_9/time_distributed_11/model_8/block1b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"380/380 [==============================] - 706s 2s/step - loss: 0.6939 - accuracy: 0.7924 - val_loss: 0.9626 - val_accuracy: 0.7586\nEpoch 2/10\n380/380 [==============================] - 582s 2s/step - loss: 0.1809 - accuracy: 0.9420 - val_loss: 0.8353 - val_accuracy: 0.8196\nEpoch 3/10\n380/380 [==============================] - 582s 2s/step - loss: 0.0994 - accuracy: 0.9733 - val_loss: 1.1433 - val_accuracy: 0.8188\nEpoch 4/10\n380/380 [==============================] - 594s 2s/step - loss: 0.1261 - accuracy: 0.9684 - val_loss: 1.3529 - val_accuracy: 0.7768\nEpoch 5/10\n380/380 [==============================] - 585s 2s/step - loss: 0.0994 - accuracy: 0.9727 - val_loss: 0.9995 - val_accuracy: 0.8377\nEpoch 6/10\n380/380 [==============================] - 602s 2s/step - loss: 0.0605 - accuracy: 0.9829 - val_loss: 1.3843 - val_accuracy: 0.8122\nEpoch 7/10\n380/380 [==============================] - 564s 1s/step - loss: 0.0434 - accuracy: 0.9868 - val_loss: 1.1564 - val_accuracy: 0.8229\nEpoch 8/10\n380/380 [==============================] - 576s 2s/step - loss: 0.0557 - accuracy: 0.9881 - val_loss: 1.4001 - val_accuracy: 0.8204\nEpoch 9/10\n380/380 [==============================] - 561s 1s/step - loss: 0.0426 - accuracy: 0.9931 - val_loss: 1.2333 - val_accuracy: 0.8204\nEpoch 10/10\n380/380 [==============================] - 582s 2s/step - loss: 0.0363 - accuracy: 0.9937 - val_loss: 1.2606 - val_accuracy: 0.8451\n","output_type":"stream"}]},{"cell_type":"code","source":"hist=model_effnet_lstm_att.fit(dataset_train, epochs=10,validation_data=dataset_test)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T18:13:28.799728Z","iopub.execute_input":"2023-04-27T18:13:28.800719Z","iopub.status.idle":"2023-04-27T19:55:51.442907Z","shell.execute_reply.started":"2023-04-27T18:13:28.800677Z","shell.execute_reply":"2023-04-27T19:55:51.441664Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Epoch 1/10\n380/380 [==============================] - 588s 2s/step - loss: 0.0826 - accuracy: 0.9786 - val_loss: 1.6860 - val_accuracy: 0.8435\nEpoch 2/10\n380/380 [==============================] - 573s 2s/step - loss: 0.0517 - accuracy: 0.9888 - val_loss: 1.4925 - val_accuracy: 0.8583\nEpoch 3/10\n380/380 [==============================] - 610s 2s/step - loss: 0.0192 - accuracy: 0.9941 - val_loss: 1.7822 - val_accuracy: 0.8402\nEpoch 4/10\n380/380 [==============================] - 640s 2s/step - loss: 0.0724 - accuracy: 0.9871 - val_loss: 1.7911 - val_accuracy: 0.8484\nEpoch 5/10\n380/380 [==============================] - 590s 2s/step - loss: 0.0381 - accuracy: 0.9951 - val_loss: 1.4279 - val_accuracy: 0.8517\nEpoch 6/10\n380/380 [==============================] - 592s 2s/step - loss: 0.0302 - accuracy: 0.9908 - val_loss: 1.6689 - val_accuracy: 0.8418\nEpoch 7/10\n380/380 [==============================] - 594s 2s/step - loss: 0.0862 - accuracy: 0.9845 - val_loss: 1.4450 - val_accuracy: 0.8361\nEpoch 8/10\n380/380 [==============================] - 597s 2s/step - loss: 0.0337 - accuracy: 0.9928 - val_loss: 1.6435 - val_accuracy: 0.8394\nEpoch 9/10\n380/380 [==============================] - 599s 2s/step - loss: 0.0084 - accuracy: 0.9987 - val_loss: 1.9197 - val_accuracy: 0.8221\nEpoch 10/10\n380/380 [==============================] - 603s 2s/step - loss: 0.0459 - accuracy: 0.9937 - val_loss: 1.7251 - val_accuracy: 0.8418\n","output_type":"stream"}]},{"cell_type":"code","source":"model_effnet_lstm_att.save(\"/kaggle/working/model_effnet_lstm_att.h5\")","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:18:35.551117Z","iopub.execute_input":"2023-04-27T20:18:35.552163Z","iopub.status.idle":"2023-04-27T20:18:38.110078Z","shell.execute_reply.started":"2023-04-27T20:18:35.552120Z","shell.execute_reply":"2023-04-27T20:18:38.108972Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}